{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from natsort import natsorted\n",
    "import math\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "positional_index = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize_documents(directory_path):\n",
    "    files_name = natsorted(os.listdir(directory_path))\n",
    "    token_lists = []\n",
    "    term_lists = []\n",
    "\n",
    "    for file_name in files_name:\n",
    "        with open(os.path.join(directory_path, file_name), \"r\") as f:\n",
    "            document = f.read()\n",
    "            tokens, terms = tokenize_and_stem(document)\n",
    "            token_lists.append(tokens)\n",
    "            term_lists.append(terms)\n",
    "\n",
    "    return token_lists, term_lists\n",
    "\n",
    "def tokenize_and_stem(doc):\n",
    "    token_docs = word_tokenize(doc)\n",
    "    prepared_tokens = [token.lower() for token in token_docs]\n",
    "    stemmed_terms = [stem(token) for token in token_docs]\n",
    "    return prepared_tokens, stemmed_terms\n",
    "\n",
    "def stem(token):\n",
    "    return PorterStemmer().stem(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['antoni', 'brutu', 'caeser', 'cleopatra', 'merci', 'worser'],\n",
       " ['antoni', 'brutu', 'caeser', 'calpurnia'],\n",
       " ['merci', 'worser'],\n",
       " ['brutu', 'caeser', 'merci', 'worser'],\n",
       " ['caeser', 'merci', 'worser'],\n",
       " ['antoni', 'caeser', 'merci'],\n",
       " ['angel', 'fool', 'fear', 'in', 'rush', 'to', 'tread', 'where'],\n",
       " ['angel', 'fool', 'fear', 'in', 'rush', 'to', 'tread', 'where'],\n",
       " ['angel', 'fool', 'in', 'rush', 'to', 'tread', 'where'],\n",
       " ['fool', 'fear', 'in', 'rush', 'to', 'tread', 'where']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_directory_path = \"docs\"\n",
    "document_of_tokens, document_of_terms = read_and_tokenize_documents(docs_directory_path)\n",
    "document_of_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSITIONAL INDEX MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_positional_index(document_of_terms):\n",
    "    for doc_id, document in enumerate(document_of_terms, start=1):\n",
    "        for position, term in enumerate(document):\n",
    "            if term not in positional_index:\n",
    "                positional_index[term] = [0, {}]\n",
    "            positional_index[term][0] += 1\n",
    "            positional_index[term][1].setdefault(doc_id, []).append(position)\n",
    "    return positional_index\n",
    "\n",
    "def get_key_by_value(dictionary, target_value, default=None):\n",
    "    \"\"\"Get the key for a given value in a dictionary.\"\"\"\n",
    "    return next((key for key, value in dictionary.items() if value == target_value), default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'antoni': [3, {1: [0], 2: [0], 6: [0]}],\n",
       " 'brutu': [3, {1: [1], 2: [1], 4: [0]}],\n",
       " 'caeser': [5, {1: [2], 2: [2], 4: [1], 5: [0], 6: [1]}],\n",
       " 'cleopatra': [1, {1: [3]}],\n",
       " 'merci': [5, {1: [4], 3: [0], 4: [2], 5: [1], 6: [2]}],\n",
       " 'worser': [4, {1: [5], 3: [1], 4: [3], 5: [2]}],\n",
       " 'calpurnia': [1, {2: [3]}],\n",
       " 'angel': [3, {7: [0], 8: [0], 9: [0]}],\n",
       " 'fool': [4, {7: [1], 8: [1], 9: [1], 10: [0]}],\n",
       " 'fear': [3, {7: [2], 8: [2], 10: [1]}],\n",
       " 'in': [4, {7: [3], 8: [3], 9: [2], 10: [2]}],\n",
       " 'rush': [4, {7: [4], 8: [4], 9: [3], 10: [3]}],\n",
       " 'to': [4, {7: [5], 8: [5], 9: [4], 10: [4]}],\n",
       " 'tread': [4, {7: [6], 8: [6], 9: [5], 10: [5]}],\n",
       " 'where': [4, {7: [7], 8: [7], 9: [6], 10: [6]}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_positional_index(document_of_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERM-FREQUANCY.TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_frequency_dataframe(document_of_tokens):\n",
    "    term_frequency = {}\n",
    "\n",
    "    for doc_id, document in enumerate(document_of_tokens, start=1):\n",
    "        for term in document:\n",
    "            term_frequency.setdefault(term, {}).setdefault(doc_id, 0)\n",
    "            term_frequency[term][doc_id] += 1\n",
    "\n",
    "    df = pd.DataFrame.from_dict(term_frequency, orient='index').fillna(0)\n",
    "    df.columns = [f'doc_{col}' for col in df.columns]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['antoni', 'brutu', 'caeser', 'cleopatra', 'merci', 'worser'],\n",
       " ['antoni', 'brutu', 'caeser', 'calpurnia'],\n",
       " ['merci', 'worser'],\n",
       " ['brutu', 'caeser', 'merci', 'worser'],\n",
       " ['caeser', 'merci', 'worser'],\n",
       " ['antoni', 'caeser', 'merci'],\n",
       " ['angel', 'fool', 'fear', 'in', 'rush', 'to', 'tread', 'where'],\n",
       " ['angel', 'fool', 'fear', 'in', 'rush', 'to', 'tread', 'where'],\n",
       " ['angel', 'fool', 'in', 'rush', 'to', 'tread', 'where'],\n",
       " ['fool', 'fear', 'in', 'rush', 'to', 'tread', 'where']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_of_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "      <th>doc_6</th>\n",
       "      <th>doc_4</th>\n",
       "      <th>doc_5</th>\n",
       "      <th>doc_3</th>\n",
       "      <th>doc_7</th>\n",
       "      <th>doc_8</th>\n",
       "      <th>doc_9</th>\n",
       "      <th>doc_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>antony</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brutus</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caeser</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleopatra</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mercy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worser</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calpurnia</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angels</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fools</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rush</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tread</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_1  doc_2  doc_6  doc_4  doc_5  doc_3  doc_7  doc_8  doc_9  \\\n",
       "antony       1.0    1.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "brutus       1.0    1.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
       "caeser       1.0    1.0    1.0    1.0    1.0    0.0    0.0    0.0    0.0   \n",
       "cleopatra    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "mercy        1.0    0.0    1.0    1.0    1.0    1.0    0.0    0.0    0.0   \n",
       "worser       1.0    0.0    0.0    1.0    1.0    1.0    0.0    0.0    0.0   \n",
       "calpurnia    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "angels       0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "fools        0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "fear         0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    0.0   \n",
       "in           0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "rush         0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "to           0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "tread        0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "where        0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "\n",
       "           doc_10  \n",
       "antony        0.0  \n",
       "brutus        0.0  \n",
       "caeser        0.0  \n",
       "cleopatra     0.0  \n",
       "mercy         0.0  \n",
       "worser        0.0  \n",
       "calpurnia     0.0  \n",
       "angels        0.0  \n",
       "fools         1.0  \n",
       "fear          1.0  \n",
       "in            1.0  \n",
       "rush          1.0  \n",
       "to            1.0  \n",
       "tread         1.0  \n",
       "where         1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq_df = create_term_frequency_dataframe(document_of_tokens)\n",
    "term_freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEIGHTED TF TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "      <th>doc_6</th>\n",
       "      <th>doc_4</th>\n",
       "      <th>doc_5</th>\n",
       "      <th>doc_3</th>\n",
       "      <th>doc_7</th>\n",
       "      <th>doc_8</th>\n",
       "      <th>doc_9</th>\n",
       "      <th>doc_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>antony</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brutus</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caeser</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleopatra</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mercy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worser</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calpurnia</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angels</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fools</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rush</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tread</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_1  doc_2  doc_6  doc_4  doc_5  doc_3  doc_7  doc_8  doc_9  \\\n",
       "antony       1.0    1.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "brutus       1.0    1.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
       "caeser       1.0    1.0    1.0    1.0    1.0    0.0    0.0    0.0    0.0   \n",
       "cleopatra    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "mercy        1.0    0.0    1.0    1.0    1.0    1.0    0.0    0.0    0.0   \n",
       "worser       1.0    0.0    0.0    1.0    1.0    1.0    0.0    0.0    0.0   \n",
       "calpurnia    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "angels       0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "fools        0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "fear         0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    0.0   \n",
       "in           0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "rush         0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "to           0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "tread        0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "where        0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    1.0   \n",
       "\n",
       "           doc_10  \n",
       "antony        0.0  \n",
       "brutus        0.0  \n",
       "caeser        0.0  \n",
       "cleopatra     0.0  \n",
       "mercy         0.0  \n",
       "worser        0.0  \n",
       "calpurnia     0.0  \n",
       "angels        0.0  \n",
       "fools         1.0  \n",
       "fear          1.0  \n",
       "in            1.0  \n",
       "rush          1.0  \n",
       "to            1.0  \n",
       "tread         1.0  \n",
       "where         1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weighted_TF(x):\n",
    "    return np.log10(x) + 1 if x > 0 else 0\n",
    "\n",
    "weighted_term_freq_df = term_freq_df.apply(lambda x: x.apply(weighted_TF))\n",
    "weighted_term_freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INVERSE DOCUMENT FREQUANCY IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_doc_frequency_doc_inv_frequency(term_freq_df):\n",
    "    number_of_docs = len(term_freq_df.columns)\n",
    "    df = term_freq_df.sum(axis=1)\n",
    "    inverse_doc_freq = np.log10(number_of_docs / df.astype(float))\n",
    "\n",
    "    idf = pd.DataFrame(\n",
    "        {\"doc_freq\": df, \"inverse_doc_freq\": inverse_doc_freq},\n",
    "        index=term_freq_df.index,\n",
    "    )\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>inverse_doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>antony</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.522879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brutus</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.522879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caeser</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleopatra</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mercy</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worser</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calpurnia</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angels</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.522879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fools</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.522879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rush</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tread</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_freq  inverse_doc_freq\n",
       "antony          3.0          0.522879\n",
       "brutus          3.0          0.522879\n",
       "caeser          5.0          0.301030\n",
       "cleopatra       1.0          1.000000\n",
       "mercy           5.0          0.301030\n",
       "worser          4.0          0.397940\n",
       "calpurnia       1.0          1.000000\n",
       "angels          3.0          0.522879\n",
       "fools           4.0          0.397940\n",
       "fear            3.0          0.522879\n",
       "in              4.0          0.397940\n",
       "rush            4.0          0.397940\n",
       "to              4.0          0.397940\n",
       "tread           4.0          0.397940\n",
       "where           4.0          0.397940"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf = calculate_doc_frequency_doc_inv_frequency(term_freq_df)\n",
    "tfdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf_idf_optimized(term_freq_df, idf):\n",
    "    term_freq_inverse_doc_freq = term_freq_df.mul(idf[\"inverse_doc_freq\"], axis=0)\n",
    "    return term_freq_inverse_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "      <th>doc_6</th>\n",
       "      <th>doc_4</th>\n",
       "      <th>doc_5</th>\n",
       "      <th>doc_3</th>\n",
       "      <th>doc_7</th>\n",
       "      <th>doc_8</th>\n",
       "      <th>doc_9</th>\n",
       "      <th>doc_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>antony</th>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brutus</th>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caeser</th>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleopatra</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mercy</th>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worser</th>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.39794</td>\n",
       "      <td>0.39794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calpurnia</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angels</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fools</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rush</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tread</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              doc_1     doc_2     doc_6     doc_4    doc_5    doc_3     doc_7  \\\n",
       "antony     0.522879  0.522879  0.522879  0.000000  0.00000  0.00000  0.000000   \n",
       "brutus     0.522879  0.522879  0.000000  0.522879  0.00000  0.00000  0.000000   \n",
       "caeser     0.301030  0.301030  0.301030  0.301030  0.30103  0.00000  0.000000   \n",
       "cleopatra  1.000000  0.000000  0.000000  0.000000  0.00000  0.00000  0.000000   \n",
       "mercy      0.301030  0.000000  0.301030  0.301030  0.30103  0.30103  0.000000   \n",
       "worser     0.397940  0.000000  0.000000  0.397940  0.39794  0.39794  0.000000   \n",
       "calpurnia  0.000000  1.000000  0.000000  0.000000  0.00000  0.00000  0.000000   \n",
       "angels     0.000000  0.000000  0.000000  0.000000  0.00000  0.00000  0.522879   \n",
       "fools      0.000000  0.000000  0.000000  0.000000  0.00000  0.00000  0.397940   \n",
       "fear       0.000000  0.000000  0.000000  0.000000  0.00000  0.00000  0.522879   \n",
       "in         0.000000  0.000000  0.000000  0.000000  0.00000  0.00000  0.397940   \n",
       "rush       0.000000  0.000000  0.000000  0.000000  0.00000  0.00000  0.397940   \n",
       "to         0.000000  0.000000  0.000000  0.000000  0.00000  0.00000  0.397940   \n",
       "tread      0.000000  0.000000  0.000000  0.000000  0.00000  0.00000  0.397940   \n",
       "where      0.000000  0.000000  0.000000  0.000000  0.00000  0.00000  0.397940   \n",
       "\n",
       "              doc_8     doc_9    doc_10  \n",
       "antony     0.000000  0.000000  0.000000  \n",
       "brutus     0.000000  0.000000  0.000000  \n",
       "caeser     0.000000  0.000000  0.000000  \n",
       "cleopatra  0.000000  0.000000  0.000000  \n",
       "mercy      0.000000  0.000000  0.000000  \n",
       "worser     0.000000  0.000000  0.000000  \n",
       "calpurnia  0.000000  0.000000  0.000000  \n",
       "angels     0.522879  0.522879  0.000000  \n",
       "fools      0.397940  0.397940  0.397940  \n",
       "fear       0.522879  0.000000  0.522879  \n",
       "in         0.397940  0.397940  0.397940  \n",
       "rush       0.397940  0.397940  0.397940  \n",
       "to         0.397940  0.397940  0.397940  \n",
       "tread      0.397940  0.397940  0.397940  \n",
       "where      0.397940  0.397940  0.397940  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = calculate_tf_idf_optimized(term_freq_df, tfdf)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSINE SIMILARITY, DOCUMENT LENGTHS, and NORMALIZATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query_vector, document_vectors):\n",
    "    query_magnitude = np.linalg.norm(query_vector)\n",
    "    document_magnitudes = np.linalg.norm(document_vectors, axis=1)\n",
    "\n",
    "    dot_products = np.dot(document_vectors, query_vector)\n",
    "    cosine_similarities = dot_products / (document_magnitudes * query_magnitude)\n",
    "\n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "def get_query_vector(query, all_words):\n",
    "    query_vector = np.zeros(len(all_words))\n",
    "    query_terms = query.split()\n",
    "\n",
    "    for term in query_terms:\n",
    "        if term in all_words:\n",
    "            query_vector[all_words.index(term)] += 1\n",
    "\n",
    "    return query_vector\n",
    "\n",
    "\n",
    "def calculate_document_lengths(term_freq_inve_doc_freq):\n",
    "    return np.sqrt((term_freq_inve_doc_freq**2).sum(axis=0))\n",
    "\n",
    "\n",
    "def normalize_term_freq_idf(term_freq_inve_doc_freq, document_lengths):\n",
    "    return term_freq_inve_doc_freq.div(document_lengths, axis=1, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc_1     1.373462\n",
       "doc_2     1.279618\n",
       "doc_6     0.674270\n",
       "doc_4     0.782941\n",
       "doc_5     0.582747\n",
       "doc_3     0.498974\n",
       "doc_7     1.223496\n",
       "doc_8     1.223496\n",
       "doc_9     1.106137\n",
       "doc_10    1.106137\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_lengths = calculate_document_lengths(tfidf)\n",
    "document_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "      <th>doc_6</th>\n",
       "      <th>doc_4</th>\n",
       "      <th>doc_5</th>\n",
       "      <th>doc_3</th>\n",
       "      <th>doc_7</th>\n",
       "      <th>doc_8</th>\n",
       "      <th>doc_9</th>\n",
       "      <th>doc_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>antony</th>\n",
       "      <td>0.380701</td>\n",
       "      <td>0.408621</td>\n",
       "      <td>0.775474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brutus</th>\n",
       "      <td>0.380701</td>\n",
       "      <td>0.408621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.667839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caeser</th>\n",
       "      <td>0.219176</td>\n",
       "      <td>0.235250</td>\n",
       "      <td>0.446453</td>\n",
       "      <td>0.384486</td>\n",
       "      <td>0.516570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleopatra</th>\n",
       "      <td>0.728087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mercy</th>\n",
       "      <td>0.219176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446453</td>\n",
       "      <td>0.384486</td>\n",
       "      <td>0.516570</td>\n",
       "      <td>0.603298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worser</th>\n",
       "      <td>0.289735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.508263</td>\n",
       "      <td>0.682869</td>\n",
       "      <td>0.797516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calpurnia</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.781483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angels</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427365</td>\n",
       "      <td>0.427365</td>\n",
       "      <td>0.472707</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fools</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427365</td>\n",
       "      <td>0.427365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rush</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tread</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              doc_1     doc_2     doc_6     doc_4     doc_5     doc_3  \\\n",
       "antony     0.380701  0.408621  0.775474  0.000000  0.000000  0.000000   \n",
       "brutus     0.380701  0.408621  0.000000  0.667839  0.000000  0.000000   \n",
       "caeser     0.219176  0.235250  0.446453  0.384486  0.516570  0.000000   \n",
       "cleopatra  0.728087  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "mercy      0.219176  0.000000  0.446453  0.384486  0.516570  0.603298   \n",
       "worser     0.289735  0.000000  0.000000  0.508263  0.682869  0.797516   \n",
       "calpurnia  0.000000  0.781483  0.000000  0.000000  0.000000  0.000000   \n",
       "angels     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "fools      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "fear       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "in         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "rush       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "to         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "tread      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "where      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "              doc_7     doc_8     doc_9    doc_10  \n",
       "antony     0.000000  0.000000  0.000000  0.000000  \n",
       "brutus     0.000000  0.000000  0.000000  0.000000  \n",
       "caeser     0.000000  0.000000  0.000000  0.000000  \n",
       "cleopatra  0.000000  0.000000  0.000000  0.000000  \n",
       "mercy      0.000000  0.000000  0.000000  0.000000  \n",
       "worser     0.000000  0.000000  0.000000  0.000000  \n",
       "calpurnia  0.000000  0.000000  0.000000  0.000000  \n",
       "angels     0.427365  0.427365  0.472707  0.000000  \n",
       "fools      0.325248  0.325248  0.359756  0.359756  \n",
       "fear       0.427365  0.427365  0.000000  0.472707  \n",
       "in         0.325248  0.325248  0.359756  0.359756  \n",
       "rush       0.325248  0.325248  0.359756  0.359756  \n",
       "to         0.325248  0.325248  0.359756  0.359756  \n",
       "tread      0.325248  0.325248  0.359756  0.359756  \n",
       "where      0.325248  0.325248  0.359756  0.359756  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_term_freq_idf = normalize_term_freq_idf(tfidf, document_lengths)\n",
    "normalized_term_freq_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST THE SEARCH ENGINE and SAVE RESULT IN TXT FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_dataframe(query_terms, normalized_term_freq_idf, tfdf, positional_index):\n",
    "    query_df = pd.DataFrame(index=normalized_term_freq_idf.index)\n",
    "    query_df[\"tf\"] = [1 if term in query_terms else 0 for term in positional_index.keys()]\n",
    "    query_df[\"w_tf\"] = query_df[\"tf\"].apply(weighted_TF)\n",
    "    query_df[\"idf\"] = tfdf[\"inverse_doc_freq\"] * query_df[\"w_tf\"]\n",
    "    query_df[\"tf_idf\"] = query_df[\"w_tf\"] * query_df[\"idf\"]\n",
    "    query_df[\"normalized\"] = query_df[\"idf\"] / np.sqrt((query_df[\"idf\"] ** 2).sum())\n",
    "    return query_df\n",
    "\n",
    "\n",
    "def calculate_product(query_df, normalized_term_freq_idf):\n",
    "    product = normalized_term_freq_idf.multiply(query_df[\"normalized\"], axis=0)\n",
    "    return product\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(product_result):\n",
    "    return product_result.sum()\n",
    "\n",
    "\n",
    "def get_related_docs(query_df, positional_index):\n",
    "    related_docs = set()\n",
    "\n",
    "    for term in query_df[query_df[\"tf\"] > 0].index:\n",
    "        if term in positional_index:\n",
    "            related_docs.update(positional_index[term][1].keys())\n",
    "\n",
    "    return list(related_docs)\n",
    "\n",
    "\n",
    "def write_to_file(file_path, content):\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(content)\n",
    "    print(\"Your results are printed :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                            PHRASE QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_positions(queries, positional_index):\n",
    "    \n",
    "    final_result = []\n",
    "    for query in queries:\n",
    "        term_lists = [[] for _ in range(10)]\n",
    "        _, query_terms = tokenize_and_stem(query)\n",
    "        if query_terms:\n",
    "            for term in query_terms:\n",
    "                if term not in positional_index:\n",
    "                    return False\n",
    "                else:\n",
    "                    for key, positions in positional_index[term][1].items():\n",
    "                        term_lists[key - 1].extend(positions)\n",
    "\n",
    "            matching_positions = [\n",
    "                f\"{pos}\"\n",
    "                for pos, positions in enumerate(term_lists, start=1)\n",
    "                if len(positions) == len(query_terms)\n",
    "            ]\n",
    "\n",
    "            final_result.append(matching_positions)\n",
    "        else:\n",
    "            return False\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"antony brutus\", \"mercy worser\"]\n",
    "matching = find_matching_positions([\"antony brutus\", \"mercy worser\"], positional_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"and\" in query:\n",
    "    set(matching[0]).intersection(set(matching[1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(matching[0]).intersection(set(matching[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2'], ['1', '3', '4', '5']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_positions(queries, positional_index):\n",
    "    final_result = []\n",
    "    for query in queries:\n",
    "        term_lists = [[] for _ in range(10)]\n",
    "        _, query_terms = tokenize_and_stem(query)\n",
    "        if query_terms:\n",
    "            for term in query_terms:\n",
    "                if term not in positional_index:\n",
    "                    return False\n",
    "                else:\n",
    "                    for key, positions in positional_index[term][1].items():\n",
    "                        term_lists[key - 1].extend(positions)\n",
    "\n",
    "            matching_positions = [\n",
    "                f\"{pos}\"\n",
    "                for pos, positions in enumerate(term_lists, start=1)\n",
    "                if len(positions) == len(query_terms)\n",
    "            ]\n",
    "\n",
    "            final_result.append(matching_positions)\n",
    "        else:\n",
    "            return False\n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_boolean_query(returned_matches_docs):\n",
    "    if not returned_matches_docs or any(not doc_set for doc_set in returned_matches_docs):\n",
    "        print(\"No matched documents.\")\n",
    "        return None\n",
    "\n",
    "    temp_result = set(returned_matches_docs[0])\n",
    "\n",
    "    for doc_set in returned_matches_docs[1:]:\n",
    "        temp_result = temp_result.intersection(set(doc_set))\n",
    "\n",
    "    return list(temp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_boolean_query(returned_matches_docs):\n",
    "    if not returned_matches_docs or any(not doc_set for doc_set in returned_matches_docs):\n",
    "        print(\"No matched documents.\")\n",
    "        return None\n",
    "\n",
    "    temp_result = set()\n",
    "\n",
    "    for doc_set in returned_matches_docs:\n",
    "        temp_result.update(doc_set)\n",
    "\n",
    "    return list(temp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_boolean_query(query_set):\n",
    "    full_set = set([\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"])\n",
    "    if not query_set:\n",
    "        print(\"No query set provided.\")\n",
    "        return list(full_set)\n",
    "\n",
    "    complement_set = full_set.difference(query_set)\n",
    "    return list(complement_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_contains_boolean_logic(query):\n",
    "    query_words = query.lower().split(\" \")\n",
    "    boolean_operator = None\n",
    "\n",
    "    if \"and\" in query_words:\n",
    "        boolean_operator = \"and\"\n",
    "    elif \"or\" in query_words:\n",
    "        boolean_operator = \"or\"\n",
    "    elif \"not\" in query_words:\n",
    "        boolean_operator = \"not\"\n",
    "    query_words = \" \".join(query_words)\n",
    "\n",
    "    return query_words, boolean_operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_query_serach(query, positional_index):\n",
    "    term_lists = [[] for _ in range(10)]\n",
    "    _, query_terms = tokenize_and_stem(query)\n",
    "    if query_terms:\n",
    "        for term in query_terms:\n",
    "            if term not in positional_index:\n",
    "                return False\n",
    "            else:\n",
    "                for key, positions in positional_index[term][1].items():\n",
    "                    term_lists[key - 1].extend(positions)\n",
    "\n",
    "        matching_positions = [\n",
    "            f\"doc_{pos}\"\n",
    "            for pos, positions in enumerate(term_lists, start=1)\n",
    "            if len(positions) == len(query_terms)\n",
    "        ]\n",
    "\n",
    "        return \", \".join(matching_positions)\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output_content(query, related_docs_PQ_stage):\n",
    "    query_tokens, query_terms = tokenize_and_stem(query)\n",
    "\n",
    "    if related_docs_PQ_stage:\n",
    "        # save the result for Phrase query\n",
    "        document_lengths = calculate_document_lengths(tfidf)\n",
    "        normalized_term_freq_idf = normalize_term_freq_idf(tfidf, document_lengths)\n",
    "        query_df = create_query_dataframe(\n",
    "            query_terms, normalized_term_freq_idf, tfdf, positional_index\n",
    "        )\n",
    "        product_result = calculate_product(query_df, normalized_term_freq_idf)\n",
    "        similarity = calculate_cosine_similarity(product_result)\n",
    "\n",
    "        try:\n",
    "            query_detailed = query_df.loc[query_tokens]\n",
    "\n",
    "            # Write results to a text file\n",
    "            results_content = f\"\"\"\n",
    "Vector Space Model for Query:\\n{query_detailed}\\n\\n\n",
    "Product Sum:\\n{(product_result.sum()).loc[related_docs_PQ_stage.split(\", \"),]}\\n\\n\n",
    "Product (query * matched doc):\\n{product_result.loc[query_tokens, related_docs_PQ_stage.split(\", \")]}\\n\\n\n",
    "Similarity:\\n{similarity.loc[related_docs_PQ_stage.split(\", \"),]}\\n\\n\n",
    "Query Length:\\n{math.sqrt(sum(query_df['idf'] ** 2))}\\n\\n\"\"\"\n",
    "            results_content += f\"\\n\\nRelated Docs:\\n{related_docs_PQ_stage}\"\n",
    "            print(results_content)\n",
    "\n",
    "        except KeyError:\n",
    "            print(f\"No such query found in the database:{query_tokens}\\nTry Again.\\n\")\n",
    "    else:\n",
    "        results_content = f\"No such query found in the database:{query_terms}\"\n",
    "        print(results_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Phrase Query stage Search for: antony brutus\n",
      "\n",
      "Vector Space Model for Query:\n",
      "        tf  w_tf       idf    tf_idf  normalized\n",
      "antony   1   1.0  0.522879  0.522879    0.707107\n",
      "brutus   1   1.0  0.522879  0.522879    0.707107\n",
      "\n",
      "\n",
      "Product Sum:\n",
      "doc_1    0.538393\n",
      "doc_2    0.577877\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Product (query * matched doc):\n",
      "           doc_1     doc_2\n",
      "antony  0.269196  0.288939\n",
      "brutus  0.269196  0.288939\n",
      "\n",
      "\n",
      "Similarity:\n",
      "doc_1    0.538393\n",
      "doc_2    0.577877\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Query Length:\n",
      "0.7394622130520805\n",
      "\n",
      "\n",
      "\n",
      "Related Docs:\n",
      "doc_1, doc_2\n",
      "If you want to EXIT enter q/Q: antony or to\n",
      "In the Phrase Query stage Search for: antony or to\n",
      "\n",
      "Vector Space Model for Query:\n",
      "        tf  w_tf       idf    tf_idf  normalized\n",
      "antony   1   1.0  0.522879  0.522879    0.795757\n",
      "to       1   1.0  0.397940  0.397940    0.605616\n",
      "\n",
      "\n",
      "Product Sum:\n",
      "doc_8     0.196976\n",
      "doc_2     0.325163\n",
      "doc_10    0.217874\n",
      "doc_6     0.617089\n",
      "doc_1     0.302946\n",
      "doc_7     0.196976\n",
      "doc_9     0.217874\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Product (query * matched doc):\n",
      "           doc_8     doc_2    doc_10     doc_6     doc_1     doc_7     doc_9\n",
      "antony  0.000000  0.325163  0.000000  0.617089  0.302946  0.000000  0.000000\n",
      "to      0.196976  0.000000  0.217874  0.000000  0.000000  0.196976  0.217874\n",
      "\n",
      "\n",
      "Similarity:\n",
      "doc_8     0.196976\n",
      "doc_2     0.325163\n",
      "doc_10    0.217874\n",
      "doc_6     0.617089\n",
      "doc_1     0.302946\n",
      "doc_7     0.196976\n",
      "doc_9     0.217874\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Query Length:\n",
      "0.6570832768894986\n",
      "\n",
      "\n",
      "\n",
      "Related Docs:\n",
      "doc_8, doc_2, doc_10, doc_6, doc_1, doc_7, doc_9\n",
      "If you want to EXIT enter q/Q: f\n",
      "In the Phrase Query stage Search for: antony and to\n",
      "No such query found in the database:['antony', 'to']\n",
      "Try Again.\n",
      "\n",
      "If you want to EXIT enter q/Q: q\n"
     ]
    }
   ],
   "source": [
    "end_search = \"\"\n",
    "while end_search not in [\"q\", \"Q\"]:\n",
    "    query = input(\"In the Phrase Query stage Search for: \")\n",
    "    query, boolean_operator = check_contains_boolean_logic(query)\n",
    "    if boolean_operator == \"and\":\n",
    "        query = query.split(\" and \")\n",
    "        returned_matches_doc = find_matching_positions(query, positional_index)\n",
    "        if returned_matches_doc:\n",
    "            returned_docs = and_boolean_query(returned_matches_doc)\n",
    "            returned_docs = \"doc_\" + \", doc_\".join(returned_docs)\n",
    "            query = \" \".join(query)\n",
    "            build_output_content(query, returned_docs)\n",
    "\n",
    "    elif boolean_operator == \"or\":\n",
    "        query = query.split(\" or \")\n",
    "        returned_matches_doc = find_matching_positions(query, positional_index)\n",
    "        if returned_matches_doc:\n",
    "            returned_docs = or_boolean_query(returned_matches_doc)\n",
    "            returned_docs = \"doc_\" + \", doc_\".join(returned_docs)\n",
    "            query = \" \".join(query)\n",
    "            build_output_content(query, returned_docs)\n",
    "    elif boolean_operator == \"not \":\n",
    "        query = query.split(\" not \")\n",
    "        returned_matches_doc = find_matching_positions(query, positional_index)\n",
    "        returned_docs = complement_boolean_query(returned_matches_doc)\n",
    "        print(returned_docs)\n",
    "    else:\n",
    "        returned_matches_doc = phrase_query_serach(query, positional_index)\n",
    "        if returned_matches_doc:\n",
    "            build_output_content(query, returned_matches_doc)\n",
    "        \n",
    "    end_search = input(\"If you want to EXIT enter q/Q: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc_1, doc_2, doc_3'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"doc_\" + \", doc_\".join(['1', '2', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
